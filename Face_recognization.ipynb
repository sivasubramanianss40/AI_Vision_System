{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e193379-2dd5-4f19-9e7e-addd1b886815",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install facenet_pytorch\n",
    "!pip install -U torch torchvision\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install opencv-python\n",
    "!pip install face_recognition\n",
    "!pip install pyttsx3\n",
    "!pip install numpy\n",
    "!pip install ultralytics \n",
    "!pip install timm\n",
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59938ef-51c7-4771-b7bc-f6006189980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import face_recognition\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create output folder\n",
    "output_folder = r\"C:\\Users\\HP\\Documents\\face\\face_data\\train\\sorna\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Step 2: Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "count = 0\n",
    "total_images = 10  # keep small for notebook\n",
    "print(\"üì∏ Face capture started...\")\n",
    "\n",
    "try:\n",
    "    while count < total_images:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"‚ùå Failed to read from webcam\")\n",
    "            break\n",
    "\n",
    "        # Resize frame for faster face detection\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small = small_frame[:, :, ::-1]\n",
    "\n",
    "        # Detect faces\n",
    "        face_locations = face_recognition.face_locations(rgb_small)\n",
    "\n",
    "        for top, right, bottom, left in face_locations:\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            # Crop the face\n",
    "            face_img = frame[top:bottom, left:right]\n",
    "\n",
    "            if face_img.size > 0:\n",
    "                filename = os.path.join(output_folder, f\"siva_{count:03d}.jpg\")\n",
    "                cv2.imwrite(filename, face_img)\n",
    "                count += 1\n",
    "\n",
    "                # Show preview in notebook\n",
    "                clear_output(wait=True)\n",
    "                pil_img = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "                display(pil_img)\n",
    "                print(f\"‚úÖ Saved {filename}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"‚èπÔ∏è Interrupted manually.\")\n",
    "\n",
    "# Step 3: Release the webcam\n",
    "cap.release()\n",
    "print(\"üéâ Face capture completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010babb7-993c-4e2e-944f-cfaec430b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. User parameters\n",
    "dataset_path = r\"C:\\Users\\HP\\Documents\\face\\face_data\\train\"  # <-- Your dataset path\n",
    "output_path = \"C:/Users/HP/Documents/face/swin_fewshot_model.pt\"\n",
    "\n",
    "# 2. Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# 3. Dataset\n",
    "dataset = ImageFolder(dataset_path, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "# 4. Swin Transformer encoder\n",
    "model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True)\n",
    "model.head = torch.nn.Identity()\n",
    "model.eval()\n",
    "\n",
    "# 5. Extract embeddings\n",
    "prototypes = {}\n",
    "with torch.no_grad():\n",
    "    for img, label in loader:\n",
    "        name = idx_to_class[label.item()]\n",
    "        emb = model(img).squeeze(0)\n",
    "        if name not in prototypes:\n",
    "            prototypes[name] = []\n",
    "        prototypes[name].append(emb)\n",
    "\n",
    "# 6. Mean embeddings = class prototypes\n",
    "for name in prototypes:\n",
    "    prototypes[name] = torch.stack(prototypes[name]).mean(dim=0)\n",
    "\n",
    "# 7. Save model + prototypes\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'prototypes': prototypes\n",
    "}, output_path)\n",
    "\n",
    "print(\"‚úÖ Few-shot face model trained and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc37acd-4efa-4ef6-87b3-ed2c2b797dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import timm\n",
    "from facenet_pytorch import MTCNN\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "model_path = \"C:/Users/HP/Documents/face/swin_fewshot_model.pt\"\n",
    "\n",
    "# Load face model\n",
    "checkpoint = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=False)\n",
    "model.head = torch.nn.Identity()\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "prototypes = checkpoint[\"prototypes\"]\n",
    "\n",
    "# Face detector\n",
    "mtcnn = MTCNN(keep_all=True, device=\"cpu\")\n",
    "\n",
    "# YOLOv8 object detector\n",
    "yolo = YOLO(\"yolov8n.pt\")  # Replace with custom model if needed\n",
    "\n",
    "# Face transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Face recognizer\n",
    "def identify_face(face_img_pil):\n",
    "    face_tensor = transform(face_img_pil).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        emb = model(face_tensor).squeeze(0)\n",
    "    dists = {name: torch.norm(emb - proto).item() for name, proto in prototypes.items()}\n",
    "    return min(dists, key=dists.get)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        face_names = []\n",
    "\n",
    "        # Detect faces\n",
    "        boxes, _ = mtcnn.detect(frame)\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                face_crop = frame[y1:y2, x1:x2]\n",
    "                if face_crop.size == 0: continue\n",
    "                face_pil = Image.fromarray(cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB))\n",
    "                name = identify_face(face_pil)\n",
    "                face_names.append(name)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "                cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "\n",
    "        # Detect objects\n",
    "        results = yolo(frame, verbose=False)\n",
    "        if results and results[0].boxes.cls.numel() > 0:\n",
    "            class_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "            object_names = list({yolo.names[cid] for cid in class_ids})\n",
    "        else:\n",
    "            object_names = []\n",
    "\n",
    "         # Smart Label Logic\n",
    "        if \"Siva\" in face_names and \"person\" in object_names and len(face_names) == 1:\n",
    "            label_text = \"Siva\"\n",
    "        elif face_names:\n",
    "            # If only Siva is in face_names\n",
    "            label_text = f\"{' & '.join(face_names)}\"\n",
    "            if object_names:\n",
    "                label_text += f\" with {', '.join([obj for obj in object_names if obj != 'person'])}\"\n",
    "        elif object_names:\n",
    "            label_text = f\"{', '.join(object_names)}\"\n",
    "        else:\n",
    "            label_text = \"No faces or objects detected\"\n",
    "\n",
    "\n",
    "        cv2.putText(frame, label_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "\n",
    "        # Convert BGR to RGB and display using matplotlib\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(rgb_frame)\n",
    "        plt.title(\"Siva with Objects\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user.\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e368700-078a-44ba-82dc-7e7b56875bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.7\n",
    ")\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ü™û Mirror the camera\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    hand_labels = []\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            label = handedness.classification[0].label  # \"Left\" or \"Right\"\n",
    "            hand_labels.append(f\"{label} Hand\")\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Add label to screen\n",
    "    if hand_labels:\n",
    "        label_text = \" | \".join(hand_labels)\n",
    "        cv2.putText(frame, label_text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"üñêÔ∏è Hand Detection (Mirror)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459e997-c386-4c55-87f1-e86bd75e9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "import timm\n",
    "from facenet_pytorch import MTCNN\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms as transforms\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "# ========== Load Face Recognition Model ==========\n",
    "model_path = \"C:/Users/HP/Documents/face/swin_fewshot_model.pt\"\n",
    "checkpoint = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=False)\n",
    "model.head = torch.nn.Identity()\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "prototypes = checkpoint[\"prototypes\"]\n",
    "\n",
    "# Face detection\n",
    "mtcnn = MTCNN(keep_all=True, device=\"cpu\")\n",
    "\n",
    "# YOLOv8\n",
    "yolo = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Face transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "def identify_face(face_img_pil):\n",
    "    face_tensor = transform(face_img_pil).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        emb = model(face_tensor).squeeze(0)\n",
    "    dists = {name: torch.norm(emb - proto).item() for name, proto in prototypes.items()}\n",
    "    return min(dists, key=dists.get)\n",
    "\n",
    "# ========== MediaPipe Hand Detection Setup ==========\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)\n",
    "\n",
    "# ========== Start Webcam ==========\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)  # Mirror view\n",
    "        face_names = []\n",
    "        hand_labels = []\n",
    "\n",
    "        # Face detection\n",
    "        boxes, _ = mtcnn.detect(frame)\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                face_crop = frame[y1:y2, x1:x2]\n",
    "                if face_crop.size == 0: continue\n",
    "                face_pil = Image.fromarray(cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB))\n",
    "                name = identify_face(face_pil)\n",
    "                face_names.append(name)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "                cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "\n",
    "        # Object detection\n",
    "        results = yolo(frame, verbose=False)\n",
    "        if results and results[0].boxes.cls.numel() > 0:\n",
    "            class_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "            object_names = list({yolo.names[cid] for cid in class_ids})\n",
    "        else:\n",
    "            object_names = []\n",
    "\n",
    "        # Hand detection\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        hand_results = hands.process(rgb_frame)\n",
    "\n",
    "        if hand_results.multi_hand_landmarks:\n",
    "            for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n",
    "                label = handedness.classification[0].label  # \"Left\" or \"Right\"\n",
    "                hand_labels.append(f\"{label} Hand\")\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Smart Label Logic\n",
    "        if \"Siva\" in face_names and \"person\" in object_names and len(face_names) == 1:\n",
    "            label_text = \"Siva\"\n",
    "        elif face_names:\n",
    "            label_text = f\"{' & '.join(face_names)}\"\n",
    "            if object_names:\n",
    "                label_text += f\" with {', '.join([obj for obj in object_names if obj != 'person'])}\"\n",
    "        elif object_names:\n",
    "            label_text = f\"{', '.join(object_names)}\"\n",
    "        else:\n",
    "            label_text = \"No faces or objects detected\"\n",
    "\n",
    "        if hand_labels:\n",
    "            label_text += f\" + {', '.join(hand_labels)}\"\n",
    "\n",
    "        # Show label\n",
    "        cv2.putText(frame, label_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "\n",
    "        # Display using matplotlib\n",
    "        rgb_show = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(rgb_show)\n",
    "        plt.title(\"Siva with Objects and Hands\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user.\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4230d393-3849-4944-8916-dbc6b50f25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "\n",
    "# --- Initialize models and paths ---\n",
    "model_path = \"C:/Users/HP/Documents/face/swin_fewshot_model.pt\"\n",
    "yolo = YOLO(\"yolov8n.pt\")  # You can replace with your custom trained model\n",
    "mtcnn = MTCNN(keep_all=True, device=\"cpu\")\n",
    "\n",
    "# Face recognition model (Swin Transformer)\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=False)\n",
    "model.head = torch.nn.Identity()\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "prototypes = checkpoint[\"prototypes\"]\n",
    "\n",
    "# Transform for face embedding\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "def identify_face(face_img_pil):\n",
    "    face_tensor = transform(face_img_pil).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        emb = model(face_tensor).squeeze(0)\n",
    "    dists = {name: torch.norm(emb - proto).item() for name, proto in prototypes.items()}\n",
    "    return min(dists, key=dists.get)\n",
    "\n",
    "# MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_names, object_names, hand_labels = [], [], []\n",
    "\n",
    "    # --- Face Detection & Recognition ---\n",
    "    boxes, _ = mtcnn.detect(frame)\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            face_crop = frame[y1:y2, x1:x2]\n",
    "            if face_crop.size == 0: continue\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB))\n",
    "            name = identify_face(face_pil)\n",
    "            face_names.append(name)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "            cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "\n",
    "    # --- Object Detection ---\n",
    "    results = yolo(frame, verbose=False)\n",
    "    if results and results[0].boxes.cls.numel() > 0:\n",
    "        class_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        object_names = list({yolo.names[cid] for cid in class_ids})\n",
    "\n",
    "    # --- Hand Detection ---\n",
    "    hand_result = hands.process(rgb)\n",
    "    if hand_result.multi_hand_landmarks:\n",
    "        for hand_landmarks, hand_info in zip(hand_result.multi_hand_landmarks, hand_result.multi_handedness):\n",
    "            label = hand_info.classification[0].label  # \"Left\" or \"Right\"\n",
    "            hand_labels.append(f\"{label} Hand\")\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # --- Smart Label Logic ---\n",
    "    label_text = \"\"\n",
    "    \n",
    "    # Remove 'person' from object list for clarity\n",
    "    clean_objects = [obj for obj in object_names if obj.lower() != \"person\"]\n",
    "    \n",
    "    if \"Siva\" in face_names:\n",
    "        label_text = \"Siva\"\n",
    "        if clean_objects:\n",
    "            label_text += f\" with {', '.join(clean_objects)}\"\n",
    "    else:\n",
    "        if face_names:\n",
    "            label_text = \" & \".join(face_names)\n",
    "            if clean_objects:\n",
    "                label_text += f\" with {', '.join(clean_objects)}\"\n",
    "        elif clean_objects:\n",
    "            label_text = f\"{', '.join(clean_objects)}\"\n",
    "    \n",
    "    # Add hand info if any\n",
    "    if hand_labels:\n",
    "        if label_text:\n",
    "            label_text += f\" showing {', '.join(hand_labels)}\"\n",
    "        else:\n",
    "            label_text = f\"{', '.join(hand_labels)}\"\n",
    "    \n",
    "    # Fallback if no detections\n",
    "    if not label_text:\n",
    "        label_text = \"No faces, hands, or objects detected\"\n",
    "\n",
    "\n",
    "    # --- Display ---\n",
    "    cv2.putText(frame, label_text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "    cv2.imshow(\"Siva Intelligent Vision\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
